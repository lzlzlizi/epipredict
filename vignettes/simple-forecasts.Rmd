---
title: "Simple forecasts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simple forecasts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

```{r pkgs}
library(epipredict)
library(epiprocess)
library(covidcast)
library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
```


In this vignette, we reproduce the simple forecasting activity described in the [`epiprocess` "Advanced sliding..." vignette](https://cmu-delphi.github.io/epiprocess/articles/advanced.html#version-aware-forecasting-revisited-1). We then go on to demonstrate a similar activity with a more advanced forecaster and on data from another source.

## Reproducing the ARX forecaster

First, we download the data and process as before (hidden).


```{r grab-epi-data, echo=FALSE}
theme_set(theme_bw())
# y <- covidcast_signals(
#   c("doctor-visits", "jhu-csse"),
#   c("smoothed_adj_cli", "confirmed_7dav_incidence_prop"),
#   start_day = "2020-06-01", 
#   end_day = "2021-12-01",
#   issues = c("2020-06-01", "2021-12-01"),
#   geo_type = "state", 
#   geo_values = c("ca", "fl")) 
# saveRDS(y, "inst/extdata/epi_archive.rds")
y <- readRDS(
  system.file("extdata", "epi_archive.rds", package = "epipredict", mustWork = TRUE)
)
 
x <- y[[1]] %>% 
  select(geo_value, time_value, version = issue, percent_cli = value) %>%
  as_epi_archive()

epix_merge(
  x, y[[2]] %>% 
    select(geo_value, time_value, version = issue, case_rate = value) %>%
    as_epi_archive(),
  all = TRUE)
```

We now make forecasts on the archive and compare to forecasts on the latest
data. 

```{r make-arx-kweek}
# Latest snapshot of data, and forecast dates
x_latest <- epix_as_of(x, max_version = max(x$DT$version))
fc_time_values <- seq(as.Date("2020-08-01"), as.Date("2021-12-01"), by = "1 month")


k_week_ahead <- function(x, ahead = 7, as_of = TRUE) {
  if (as_of) {
    x %>%
      epix_slide(fc = arx_forecaster(
        percent_cli, case_rate, geo_value, time_value, 
        args = arx_args_list(ahead = ahead,intercept = FALSE)),
        n = 120, ref_time_values = fc_time_values) %>%
      mutate(target_date = time_value + ahead, as_of = as_of,geo_value = fc_key_vars)
  } else {
    x_latest %>%
      epi_slide(fc = arx_forecaster(
        percent_cli, case_rate, geo_value, time_value,
        args = arx_args_list(ahead = ahead,intercept = FALSE)),
        n = 120, ref_time_values = fc_time_values) %>%
      mutate(target_date = time_value + ahead, as_of = as_of)
  }
}

# Generate the forecasts, and bind them together
fc <- bind_rows(k_week_ahead(x, ahead = 7, as_of = TRUE),
                k_week_ahead(x, ahead = 14, as_of = TRUE),
                k_week_ahead(x, ahead = 21, as_of = TRUE),
                k_week_ahead(x, ahead = 28, as_of = TRUE),
                k_week_ahead(x, ahead = 7, as_of = FALSE),
                k_week_ahead(x, ahead = 14, as_of = FALSE),
                k_week_ahead(x, ahead = 21, as_of = FALSE),
                k_week_ahead(x, ahead = 28, as_of = FALSE))
```

Here, `arx_forecaster()` does all the heavy lifting. It creates leads of the target (respecting time stamps and locations) along with lags of the features (here, the response and doctors visits), estimates an autoregressive model, creates predictions, and non-parametric confidence bands.

All of these are tunable parameters.

Now we plot them on top of the latest case rates.

```{r plot-arx, message = FALSE, warning = FALSE, fig.width = 9, fig.height = 6}
ggplot(fc, aes(x = target_date, group = time_value, fill = as_of)) +
  geom_line(data = x_latest, aes(x = time_value, y = case_rate),
            inherit.aes = FALSE, color = "gray50") +
  geom_ribbon(aes(ymin = fc_q0.05, ymax = fc_q0.95), alpha = 0.4) +
  geom_line(aes(y = fc_point)) + geom_point(aes(y = fc_point), size = 0.5) +
  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +
  facet_grid(vars(geo_value), vars(as_of), scales = "free") +
  scale_x_date(minor_breaks = "month", date_labels = "%b %y") +
  labs(x = "Date", y = "Reported COVID-19 case rates") +
  theme(legend.position = "none")
```

These look generally not great, but that's because we've only used two locations, and they're behaviour is rather different.

## Smooth forecasts at daily horizons

Because we are making forecasts for multiple horizons, we may want these to be "smooth" rather than jagged as above. One way to do this, described in [Tuzhilina et al.](https://arxiv.org/abs/2202.09723) is to estimate a version of the multiple least squares model where the response is a vector $Y \in \mathbb{R}^{d}$ with $d$ the number of horizons. So, for example, taking $h = {7, 14, 21, 28}$ as above, would result in $d=4$. By concatenating these row-wise into a matrix $\mathbf{Y}$, multiple least squares solves $d$ OLS problems simultaneously by optimizing
$$
\min_\Theta \lVert \mathbf{Y} - \mathbf{X}\Theta \rVert_F^2
$$
where $\lVert\mathbf{A}\rVert_F$ is the Frobenius norm of the matrix $\mathbf{A}$ given by $\left(\sum_{ij} a_{ij}^2\right)^{1/2}$ and $\Theta$ is a matrix of coefficients in $\mathbb{R}^{p\times d}$. 

To produce smooth forecasts, we first expand the vector of horizons $h$ in some basis (say the basis of $a$ polynomials, with $a\leq d$) and then right multiply $\mathbf{Y}$ by the result. This leads to the following smoothed optimization problem
$$
\min_\Gamma \lVert \mathbf{Y}\mathbf{H}^\mathsf{T} - \mathbf{X}\Gamma \rVert_F^2.
$$
Predictions can then be produced easily by undoing the transformation with $\mathbf{H}$. See [Tuzhilina et al.](https://arxiv.org/abs/2202.09723) for more details.

In `epipredict`, this methodology is implemented with the `smooth_arx_forecaster()`. Below, we'll again make forecasts on the archive, but this time for `h=1:28` and `a=4`. 

```{r smooth-on-the-archive}
fc_data <- x %>%
  epix_slide(
    fc = smooth_arx_forecaster(percent_cli, case_rate, geo_value, time_value) %>%
      nest_by(key_vars), # on each date, this produces a data frame, 
                         # which we nest to allow for sliding.
    n = 120, ref_time_values = fc_time_values) %>%
  unnest(fc_data) %>% # unnest it to get a long dataframe like before
  mutate(target_date = time_value + ahead) %>%
  rename(geo_value = fc_key_vars)
```

Everything else works similarly to `arx_forecaster()` above.

Unfortunately, there's a bug in this forecaster...


```{r plot-smooth, message = FALSE, warning = FALSE, fig.width = 9, fig.height = 6}
ggplot(fc_data, aes(x = target_date, group = time_value)) +
  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +
  geom_line(data = x_latest, aes(x = time_value, y = case_rate),
            inherit.aes = FALSE, color = "gray50") +
  geom_ribbon(aes(ymin = q0.05, ymax = q0.95, fill = geo_value), alpha = 0.4) +
  geom_line(aes(y = point)) +
  geom_point(aes(y = point), size = 0.5) +
  facet_wrap(~ geo_value, scales = "free_y") +
  scale_x_date(minor_breaks = "month", date_labels = "%b %y") +
  labs(x = "Date", y = "Reported COVID-19 case rates") +
  theme(legend.position = "none")
```

## Using data for Canada

By leveraging the flexibility of `epiprocess`, we can apply the same techniques to data from other sources. Since I'm in British Columbia, may as well do the same thing for Canada.

The [COVID-19 Canada Open Data Working Group](https://opencovid.ca/) collects daily time series data on COVID-19 cases, deaths, recoveries, testing and vaccinations at the health region and province levels. Data are collected from publicly available sources such as government datasets and news releases. Unfortunately, there is no simple versioned source, so we have created our own from the Commit history.

First, we load versioned case numbers at the provincial level, and convert these to an `epi_archive` object. Then we run a very similar forcasting exercise as that above.

```{r get-can-fc}
# source("drafts/canada-case-rates.R)
can <- readRDS(
  system.file("extdata", "can_prov_cases.rds", package = "epipredict", mustWork = TRUE)
  ) %>%
  group_by(version, geo_value) %>% 
  arrange(time_value) %>% 
  mutate(cr_7dav = RcppRoll::roll_meanr(case_rate, n = 7L)) #%>%
  #filter(geo_value %in% c('Alberta', "BC"))
can <- as_epi_archive(can)
can_latest <- epix_as_of(can, max_version = max(can$DT$version))

can_k_week_ahead <- function(x, ahead = 7, as_of = TRUE) {
  if(as_of){
    can %>%
      epix_slide(fc = arx_forecaster(
        y = cr_7dav, key_vars = geo_value, time_value = time_value,
        args = arx_args_list(intercept = FALSE,ahead = ahead)),
        n = 120, ref_time_values = fc_time_values) %>%
      mutate(target_date = time_value + ahead, geo_value = fc_key_vars, as_of = as_of)
  }
  else{
    can_latest %>%
      epi_slide(fc = arx_forecaster(
        y = cr_7dav, key_vars = geo_value, time_value = time_value,
        args = arx_args_list(intercept = FALSE,ahead = ahead)),
        n = 120, ref_time_values = fc_time_values) %>%
      mutate(target_date = time_value + ahead, geo_value = fc_key_vars, as_of = as_of)
  }
}

can_fc <- bind_rows(purrr:::map_dfr(c(7,14,21,28), ~ can_k_week_ahead(can, ahead = .x, as_of = TRUE)),
                    purrr:::map_dfr(c(7,14,21,28), ~ can_k_week_ahead(can_latest, ahead = .x, as_of = FALSE)))
```

The figure below shows the results for all of the provinces. Note that we are showing the 7-day averages rather than the reported case numbers due to highly variable provincial reporting mismatches.

```{r plot-can-fc, message = FALSE, warning = FALSE, fig.width = 9, fig.height = 12}
ggplot(can_fc, aes(x = target_date, group = time_value)) +
  coord_cartesian(xlim = lubridate::ymd(c("2020-12-01", NA))) +
  geom_line(data = can_latest, aes(x = time_value, y = cr_7dav),
            inherit.aes = FALSE, color = "gray50") +
  geom_ribbon(aes(ymin = fc_q0.05, ymax = fc_q0.95, fill = geo_value), alpha = 0.4) +
  geom_line(aes(y = fc_point)) + geom_point(aes(y = fc_point), size = 0.5) +
  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +
  facet_wrap(paste('as_of: ', as_of)~geo_value,scales = "free",ncol = 4) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %y") +
  labs(x = "Date", y = "Reported COVID-19 case rates") +
  theme(legend.position = "none")  
```

## Goals for `epipredict`

**We hope to provide:**

1. A set of basic, easy-to-use forecasters that work out of the box. You should be able to do a reasonably limited amount of customization on them. (Any serious customization happens with point number 2.) For the basic forecasters, we should provide, at least: 
    * Baseline flat-line forecaster 
    * Autoregressive forecaster
    * Autoregressive classifier
2. A framework for creating custom forecasters out of modular components. There are four types of components:
    * Preprocessor: do things to the data before model training
    * Trainer: train a model on data, resulting in a fitted model object
    * Predictor: make predictions, using a fitted model object
    * Postprocessor: do things to the predictions before returning

**Target audience:**

* Basic. Has data, calls forecaster with default arguments.
* Intermediate. Wants to examine changes to the arguments, take advantage of built in flexibility.
* Advanced. Wants to write their own forecasters. Maybe willing to build up from some components that we write. 

The Advanced user should find their task to be relatively easy (and we'll show them how). 

Example: During a quiet period, a user decides they want to first predict whether a surge is about to occur, say using variant information from GISAID. Then for surging locations, they want to train an AR model using past surges in the same location. Everywhere else, they predict a flat line. We should be able to do this in a few lines of code.

Delphi's own forecasts have been produced/evaluated in this way for a while now, but the code base is scattered and evolving. We want to consolidate, generalize, and simplify to allow others to benefit as well.

The basic framework should allow for something like the following. This would
feel very familiar to anyone working in `R`+`{tidyverse}`.

**Simple linear autoregressive model with scaling (modular)**

```{r ideal-framework, eval=FALSE}
my_fcaster = new_epi_predictor() %>%
  add_preprocessor(scaler, var = cases, by = pop) %>%
  add_preprocessor(lagger, var = dv_cli, lags = c(0, 7, 14)) %>%
  add_trainer(lm) %>%
  add_predictor(lm.predict) %>%
  add_postprocessor(scaler, by = 1/pop)
```

Then you could run this on an `epi_df` with one line.

```{r run-ideal, eval=FALSE}
my_fcaster(lead(cases, 7) ~ ., epi_df, key_vars, time_vars)
```

The hypothetical example of first classifying, then fitting different models would also fit into this framework. And this isn't far from our current production models.

### Why doesn't this exist

Closest neighbor is [`{fable}`](https://fable.tidyverts.org/). It does some of what we want but has a few major downsides:

1. Small set of standard Time Series models.
    * Small modifications are hard (e.g. can't "just use" `glmnet` instead of `lm`) in an AR model.
1. Multi-period forecasting is model-based only.
    * This is "iterative" forecasting, and is very bad in epidemiology.
    * Much better with simple models to use "direct" forecasting.
1. Confidence bands are model-based only.
    * In epi tasks, these dramatically under-cover.
1. Layering is not possible/natural
1. Can't use methods that aren't already implemented.

The forecasts we did above can't be produced with `{fable}`.


**However:** The developers behind `{fable}` wrote a package called `{fabletools}` that powers model creation (based on `R6`). We can almost certainly borrow some of that technology to lever up.

### What this isn't

This is not a framework for SIR models. We intend to create some simple versions, but advanced models---those that use variants, hospitalizations, different types of immunity, age stratification, etc.---cannot be compartmentalized in the same way (though see [pypm](https://pypm.github.io/home/)). These types of models also are better at scenario modeling than short term forecasts unless they are quite complicated.
